X_test_df = X_test_df,
num_trees = 200)
} else if (model == "rotation_forest"){
fit = rotation_forest_wrapper(
Y_train,
X_train_df,
X_test_df,
n_chains = 1)
}
if (model %in% c("rot_rand_forest", "rot_extra_trees")) {
accuracy_train = MLmetrics::Accuracy(fit$train[[1]] >= .5, Y_train)
accuracy_test = MLmetrics::Accuracy(fit$test[[1]] >= .5, Y_test)
auc_train = MLmetrics::AUC(fit$train[[1]] >= .5, Y_train)
auc_test = MLmetrics::AUC(fit$test[[1]] >= .5, Y_test)
recall_train = MLmetrics::Recall(Y_train, as.numeric(fit$train[[1]] >= .5))
recall_test = MLmetrics::Recall(Y_test, as.numeric(fit$test[[1]] >= .5))
precision_train = MLmetrics::Precision(Y_train, (as.numeric(fit$train[[1]] >= .5)))
precision_test = MLmetrics::Precision(Y_test, as.numeric(fit$test[[1]] >= .5))
} else{
accuracy_train = MLmetrics::Accuracy(fit$train[,"MEAN"] >= .5, Y_train)
accuracy_test = MLmetrics::Accuracy(fit$test[,"MEAN"] >= .5, Y_test)
auc_train = MLmetrics::AUC(fit$train[,"MEAN"] >= .5, Y_train)
auc_test = MLmetrics::AUC(fit$test[,"MEAN"] >= .5, Y_test)
recall_train = MLmetrics::Recall(Y_train, as.numeric(fit$train[,"MEAN"] >= .5))
recall_test = MLmetrics::Recall(Y_test, as.numeric(fit$test[,"MEAN"] >= .5))
precision_train = MLmetrics::Precision(Y_train, as.numeric(fit$train[,"MEAN"] >= .5))
precision_test = MLmetrics::Precision(Y_test, as.numeric(fit$test[,"MEAN"] >= .5))
}
if (model %in% c("rotation_forest",
"rot_rand_forest", "rot_extra_trees")){
mean_int_len_train = NA
mean_int_len_test = NA
log_loss_test = MLmetrics::LogLoss(fit$test[[1]], Y_test)
log_loss_train = MLmetrics::LogLoss(fit$train[[1]], Y_train)
} else {
mean_int_len_train = mean(fit$train[,"U95"] - fit$train[,"L95"])
mean_int_len_test = mean(fit$test[,"U95"] - fit$test[,"L95"])
log_loss_test = MLmetrics::LogLoss(fit$test[,"MEAN"], Y_test)
log_loss_train = MLmetrics::LogLoss(fit$train[,"MEAN"], Y_train)
}
mean_train_time = mean(fit$timing)
results = list(job_id = job_id,
model = model,
data = data,
split = split,
accuracy_train = accuracy_train,
accuracy_test = accuracy_test,
recall_train = recall_train,
recall_test = recall_test,
precision_train = precision_train,
precision_test = precision_test,
auc_train = auc_train,
auc_test = auc_test,
log_loss_test = log_loss_test,
log_loss_train = log_loss_train,
mean_int_len_train = mean_int_len_train,
mean_int_len_test = mean_int_len_test,
mean_train_time = mean_train_time
)
print(results)
# Author: Paul Nguyen
# Date: August 14, 2024
# Purpose: run real dataset bake-off to determine gains from obliqueBART vs other rotated axis aligned ensembles
# Details:
# Dependencies: obliqueBART, flexBART, BART, randomForest, dplyr
library(dplyr)
study = "bakeoff"
data_dir = "data/"
data_dir = "../../benchmark_datasets/regression/data/"
script_dir = "study/"
source(paste0(script_dir, "rotate_data.R"))
source(paste0(script_dir, "obliqueBART_wrapper.R"))
source(paste0(script_dir, "rot_extra_trees_reg_wrapper.R"))
source(paste0(script_dir, "rot_randForest_reg_wrapper.R"))
args = 30
job_id = as.numeric(args[1]) + 1
source(paste0(script_dir, "settings_regression.R"))
model = as.character(settings$model[job_id])
data = as.character(settings$data[job_id])
split = as.integer(settings$split[job_id])
### load data ###
load(paste0(data_dir, data, "_data.RData"))
# train split
Y_train = Y[-test_split_list[[split]]]
X_cont_train = X_cont[-test_split_list[[split]], ]
# test split
Y_test = Y[test_split_list[[split]]]
model
m
model = "rot_extra_trees"
# train split
Y_train = Y[-test_split_list[[split]]]
X_cont_train = X_cont[-test_split_list[[split]], ]
# test split
Y_test = Y[test_split_list[[split]]]
X_cont_test = X_cont[test_split_list[[split]], ]
if (!is.null(cat_level_list)){
# some data sets only have one categorical variable
# but rffbart and flexbart require a matrix input
if ((ncol(X_cat) > 1)){
X_cat_train = X_cat[-test_split_list[[split]], ]
X_cat_test = X_cat[test_split_list[[split]], ]
} else if (nrow(X_cat) > 1) {
X_cat_train = matrix(X_cat[-test_split_list[[split]], ], ncol = 1)
X_cat_test = matrix(X_cat[test_split_list[[split]], ], ncol = 1)
} else if (nrow(X_cat == 1)){
X_cat_train = matrix(0L, nrow = 1, ncol = 1)
X_cat_test = matrix(0L, nrow = 1, ncol = 1)
}
} else {
X_cat_train = matrix(0L, nrow = 1, ncol = 1)
X_cat_test = matrix(0L, nrow = 1, ncol = 1)
}
# one-hot encode categorical variables
if (sum(dim(X_cat)) > 2) {
tmp_df = as.data.frame(X_cat) %>% mutate(across(everything(), as.factor))
one_hot_cat = dummy::dummy(tmp_df) %>% mutate(across(everything(), as.integer))
X_cat_train = one_hot_cat[-test_split_list[[split]], ]
X_cat_test = one_hot_cat[test_split_list[[split]], ]
X_train = dplyr::bind_cols(X_cont_train, X_cat_train) %>% as.matrix()
X_test = dplyr::bind_cols(X_cont_test, X_cat_test) %>% as.matrix()
} else{
X_train = X_cont_train
X_test = X_cont_test
}
#change this to pbart for classification
if (model %in% c("rot_rand_forest", "extra_trees")){
# df
X_train_df = data.frame(X_train)
X_test_df = data.frame(X_test)
} else{
X_train_df = as.matrix(X_train)
X_test_df = as.matrix(X_test)
}
### fit model ###
n_chains = 1
if (model == "obart1.5a"){
fit = obliqueBART_wrapper(
Y_train = Y_train,
X_cont_train = X_train_df,
X_cont_test = X_test_df,
prob_aa = 0.5,
adaptive_prob_aa_option = T,
phi_option = 1,
n_chains = n_chains
)
} else if (model == "rot_extra_trees"){
fit = rot_extra_trees_reg_wrapper (Y_train,
Y_test,
X_train_df,
X_test_df,
num_trees = 200
)
}  else if (model == "rot_rand_forest"){
fit = rot_randForest_reg_wrapper(Y_train = Y_train,
X_train_df = X_train_df,
X_test_df = X_test_df,
num_trees = 200)
}
rmse_train = MLmetrics::RMSE(Y_train, fit$train$fit[,"MEAN"])
rmse_test = MLmetrics::RMSE(Y_test, fit$test$fit[,"MEAN"])
# randomForest does not do uncertainty quantification
if (!(model %in% c("wbart", "obart1.5a")  )){
ystar_rmse_train = NA
ystar_rmse_test = NA
ystar_cov_train = NA
ystar_cov_test = NA
mean_int_len_train = NA
mean_int_len_test = NA
} else {
ystar_rmse_train = MLmetrics::RMSE(Y_train, fit$train$ystar[,"MEAN"])
ystar_rmse_test = MLmetrics::RMSE(Y_test, fit$test$ystar[,"MEAN"])
ystar_cov_train = mean( (Y_train >= fit$train$ystar[,"L95"] & Y_train <= fit$train$ystar[,"U95"]) )
ystar_cov_test = mean( (Y_test >= fit$test$ystar[,"L95"] & Y_test <= fit$test$ystar[,"U95"]) )
mean_int_len_train = mean(fit$train$ystar[,"U95"] - fit$train$ystar[,"L95"])
mean_int_len_test = mean(fit$test$ystar[,"U95"] - fit$test$ystar[,"L95"])
}
mean_train_time = mean(fit$timing)
results = list(job_id = job_id,
model = model,
data = data,
split = split,
rmse_train = rmse_train,
rmse_test = rmse_test,
ystar_rmse_train = ystar_rmse_train,
ystar_rmse_test = ystar_rmse_test,
ystar_cov_train = ystar_cov_train,
ystar_cov_test = ystar_cov_test,
# mean_int_len_train = mean_int_len_train,
# mean_int_len_test = mean_int_len_test,
mean_train_time = mean_train_time
# mean_accept_rate = mean_accept_rate,
# mean_tree_depth = mean_tree_depth
)
results
# Author: Paul Nguyen
# Date: August 20, 2024
# Purpose: run real dataset bake-off to determine gains from obliqueBART vs other axis aligned ensembles
# Details:
# Dependencies: obliqueBART, flexBART, BART, randomForest, dplyr
library(dplyr)
study = "bakeoff"
data_dir = "data/"
# data_dir = "../../benchmark_datasets/classification/data/"
script_dir = "study/"
source(paste0(script_dir, "rotate_data.R"))
source(paste0(script_dir, "probit_oblique_BART_wrapper.R"))
source(paste0(script_dir, "rot_extra_trees_class_wrapper.R"))
source(paste0(script_dir, "rot_randForest_class_wrapper.R"))
source(paste0(script_dir, "rotation_forest_class_wrapper.R"))
### simulation settings ###
args = commandArgs(TRUE)
job_id = as.numeric(args[1]) + 1
source(paste0(script_dir, "settings_classification.R"))
View(settings)
# Author: Paul Nguyen
# Date: August 14, 2024
# Purpose: run real dataset bake-off to determine gains from obliqueBART vs other rotated axis aligned ensembles
# Details:
# Dependencies: obliqueBART, flexBART, BART, randomForest, dplyr
library(dplyr)
study = "bakeoff"
data_dir = "data/"
# data_dir = "../../benchmark_datasets/regression/data/"
script_dir = "study/"
source(paste0(script_dir, "rotate_data.R"))
source(paste0(script_dir, "obliqueBART_wrapper.R"))
source(paste0(script_dir, "rot_extra_trees_reg_wrapper.R"))
source(paste0(script_dir, "rot_randForest_reg_wrapper.R"))
### simulation settings ###
args = commandArgs(TRUE)
job_id = as.numeric(args[1]) + 1
source(paste0(script_dir, "settings_regression.R"))
model = as.character(settings$model[job_id])
# Author: Paul Nguyen
# Date: August 14, 2024
# Purpose: run real dataset bake-off to determine gains from obliqueBART vs other axis aligned ensembles
# Details:
# Dependencies: obliqueBART, flexBART, BART, randomForest, dplyr
library(dplyr)
study = "bakeoff"
data_dir = "data/"
data_dir = "../../benchmark_datasets/regression/data/"
# data_dir = "../../benchmark_datasets/regression/data/"
script_dir = "study/"
source(paste0(script_dir, "obliqueBART_wrapper.R"))
source(paste0(script_dir, "wbart_wrapper.R"))
source(paste0(script_dir, "obliqueBART_oh_wrapper.R"))
source(paste0(script_dir, "extra_trees_reg_wrapper.R"))
source(paste0(script_dir, "xgboost_reg_wrapper.R"))
source(paste0(script_dir, "extra_trees_reg_wrapper.R"))
getwd()
setwd("~/school/wisconsin/BART/bart_playground/aa_vs_obl_bakeoff")
source(paste0(script_dir, "obliqueBART_wrapper.R"))
source(paste0(script_dir, "wbart_wrapper.R"))
source(paste0(script_dir, "obliqueBART_oh_wrapper.R"))
source(paste0(script_dir, "extra_trees_reg_wrapper.R"))
source(paste0(script_dir, "xgboost_reg_wrapper.R"))
source(paste0(script_dir, "randForest_reg_wrapper.R"))
source(paste0(script_dir, "flexBART_wrapper"))
source(paste0(script_dir, "flexBART_wrapper.R"))
### simulation settings ###
args = commandArgs(TRUE)
args = 0
job_id = as.numeric(args[1]) + 1
source(paste0(script_dir, "settings_regression.R"))
model = as.character(settings$model[job_id])
data = as.character(settings$data[job_id])
split = as.integer(settings$split[job_id])
### load data ###
load(paste0(data_dir, data, "_data.RData"))
m
model = "flexBART"
# train split
Y_train = Y[-test_split_list[[split]]]
X_cont_train = X_cont[-test_split_list[[split]], ]
# test split
Y_test = Y[test_split_list[[split]]]
X_cont_test = X_cont[test_split_list[[split]], ]
if (!is.null(cat_level_list)){
# some data sets only have one categorical variable
# but rffbart and flexbart require a matrix input
if ((ncol(X_cat) > 1)){
X_cat_train = X_cat[-test_split_list[[split]], ]
X_cat_test = X_cat[test_split_list[[split]], ]
} else if (nrow(X_cat) > 1) {
X_cat_train = matrix(X_cat[-test_split_list[[split]], ], ncol = 1)
X_cat_test = matrix(X_cat[test_split_list[[split]], ], ncol = 1)
} else if (nrow(X_cat == 1)){
X_cat_train = matrix(0L, nrow = 1, ncol = 1)
X_cat_test = matrix(0L, nrow = 1, ncol = 1)
}
} else {
X_cat_train = matrix(0L, nrow = 1, ncol = 1)
X_cat_test = matrix(0L, nrow = 1, ncol = 1)
}
# combine continuous predictors and categorical predictors into either a
#   matrix or a df depending on the model
if (model %in% c("wbart", "rand_forest", "extra_trees", "gbm")){
# df
X_train_df = data.frame(dplyr::bind_cols(X_cont_train, X_cat_train))
X_test_df = data.frame(dplyr::bind_cols(X_cont_test, X_cat_test))
}
n_chains
### fit model ###
n_chains = 1
cat_levels_list
### fit model ###
n_chains = 1
if (model == "axis-aligned"){
fit = obliqueBART_wrapper(
Y_train = Y_train,
X_cont_train = X_cont_train,
X_cont_test = X_cont_test,
X_cat_train = X_cat_train,
X_cat_test = X_cat_test,
cat_levels_list = cat_level_list,
prob_aa = 1,
phi_option = 1,
n_chains = n_chains
)
}  else if (model == "wbart"){
fit = wbart_wrapper(
Y_train = Y_train,
X_train_df = X_train_df,
X_test_df = X_test_df,
n_chains = n_chains
)
} else if (model == "obart1.5a"){
fit = obliqueBART_wrapper(
Y_train = Y_train,
X_cont_train = X_cont_train,
X_cont_test = X_cont_test,
X_cat_train = X_cat_train,
X_cat_test = X_cat_test,
cat_levels_list = cat_level_list,
prob_aa = 0.5,
adaptive_prob_aa_option = T,
phi_option = 1,
n_chains = n_chains
)
} else if (model == "obart1.5a_oh"){
fit = obliqueBART_oh_wrapper(
Y_train = Y_train,
X_cont_train = X_cont_train,
X_cont_test = X_cont_test,
X_cat = X_cat,
prob_aa = 0.5,
adaptive_prob_aa_option = T,
phi_option = 1,
n_chains = n_chains
)
} else if (model == "xgboost"){
fit = xgb_reg_wrapper( Y_train = Y_train,
X_cont_train = X_cont_train,
X_cont_test = X_cont_test,
X_cat = X_cat,
test_split_list = test_split_list,
split = split,
n_chains = 1)
} else if (model == "extra_trees"){
fit = extra_trees_reg_wrapper(Y_train = Y_train,
X_train_df = X_train_df,
X_test_df = X_test_df,
n_chains = 1)
} else if (model == "rand_forest"){
fit = randForest_reg_wrapper(Y_train = Y_train,
X_train_df = X_train_df,
X_test_df = X_test_df,
n_chains = 1)
} else if (model == "flexBART"){
fit = flexBART_wrapper (
Y_train,
X_cont_train = X_cont_train,
X_cat_train = X_cat_train,
X_cont_test = X_cont_test,
X_cat_test = X_cat_test,
cat_levels_list = cat_level_list
)
}
n_chains
### fit model ###
n_chains = 1
if (model == "axis-aligned"){
fit = obliqueBART_wrapper(
Y_train = Y_train,
X_cont_train = X_cont_train,
X_cont_test = X_cont_test,
X_cat_train = X_cat_train,
X_cat_test = X_cat_test,
cat_levels_list = cat_level_list,
prob_aa = 1,
phi_option = 1,
n_chains = n_chains
)
}  else if (model == "wbart"){
fit = wbart_wrapper(
Y_train = Y_train,
X_train_df = X_train_df,
X_test_df = X_test_df,
n_chains = n_chains
)
} else if (model == "obart1.5a"){
fit = obliqueBART_wrapper(
Y_train = Y_train,
X_cont_train = X_cont_train,
X_cont_test = X_cont_test,
X_cat_train = X_cat_train,
X_cat_test = X_cat_test,
cat_levels_list = cat_level_list,
prob_aa = 0.5,
adaptive_prob_aa_option = T,
phi_option = 1,
n_chains = n_chains
)
} else if (model == "obart1.5a_oh"){
fit = obliqueBART_oh_wrapper(
Y_train = Y_train,
X_cont_train = X_cont_train,
X_cont_test = X_cont_test,
X_cat = X_cat,
prob_aa = 0.5,
adaptive_prob_aa_option = T,
phi_option = 1,
n_chains = n_chains
)
} else if (model == "xgboost"){
fit = xgb_reg_wrapper( Y_train = Y_train,
X_cont_train = X_cont_train,
X_cont_test = X_cont_test,
X_cat = X_cat,
test_split_list = test_split_list,
split = split,
n_chains = 1)
} else if (model == "extra_trees"){
fit = extra_trees_reg_wrapper(Y_train = Y_train,
X_train_df = X_train_df,
X_test_df = X_test_df,
n_chains = 1)
} else if (model == "rand_forest"){
fit = randForest_reg_wrapper(Y_train = Y_train,
X_train_df = X_train_df,
X_test_df = X_test_df,
n_chains = 1)
} else if (model == "flexBART"){
fit = flexBART_wrapper (
Y_train,
X_cont_train = X_cont_train,
X_cat_train = X_cat_train,
X_cont_test = X_cont_test,
X_cat_test = X_cat_test,
cat_levels_list = cat_level_list,
n_chains = n_chains
)
}
rmse_train = MLmetrics::RMSE(Y_train, fit$train$fit[,"MEAN"])
rmse_test = MLmetrics::RMSE(Y_test, fit$test$fit[,"MEAN"])
# randomForest does not do uncertainty quantification
if (!(model %in% c("wbart", "obart1.5a","obart1.5a_oh")  )){
ystar_rmse_train = NA
ystar_rmse_test = NA
ystar_cov_train = NA
ystar_cov_test = NA
mean_int_len_train = NA
mean_int_len_test = NA
} else {
ystar_rmse_train = MLmetrics::RMSE(Y_train, fit$train$ystar[,"MEAN"])
ystar_rmse_test = MLmetrics::RMSE(Y_test, fit$test$ystar[,"MEAN"])
ystar_cov_train = mean( (Y_train >= fit$train$ystar[,"L95"] & Y_train <= fit$train$ystar[,"U95"]) )
ystar_cov_test = mean( (Y_test >= fit$test$ystar[,"L95"] & Y_test <= fit$test$ystar[,"U95"]) )
mean_int_len_train = mean(fit$train$ystar[,"U95"] - fit$train$ystar[,"L95"])
mean_int_len_test = mean(fit$test$ystar[,"U95"] - fit$test$ystar[,"L95"])
}
mean_train_time = mean(fit$timing)
results = list(job_id = job_id,
model = model,
data = data,
split = split,
rmse_train = rmse_train,
rmse_test = rmse_test,
ystar_rmse_train = ystar_rmse_train,
ystar_rmse_test = ystar_rmse_test,
ystar_cov_train = ystar_cov_train,
ystar_cov_test = ystar_cov_test,
# mean_int_len_train = mean_int_len_train,
# mean_int_len_test = mean_int_len_test,
mean_train_time = mean_train_time
# mean_accept_rate = mean_accept_rate,
# mean_tree_depth = mean_tree_depth
)
results
# Author: Paul Nguyen
# Date: August 20, 2024
# Purpose: run real dataset bake-off to determine gains from obliqueBART vs other axis aligned ensembles
# Details:
# Dependencies: obliqueBART, flexBART, BART, randomForest, dplyr
library(dplyr)
study = "bakeoff"
data_dir = "data/"
# data_dir = "../../benchmark_datasets/classification/data/"
script_dir = "study/"
source(paste0(script_dir, "probit_oblique_BART_wrapper.R"))
source(paste0(script_dir, "pbart_wrapper.R"))
source(paste0(script_dir, "probit_oblique_BART_oh_wrapper.R"))
source(paste0(script_dir, "extra_trees_class_wrapper.R"))
source(paste0(script_dir, "xgboost_class_wrapper.R"))
source(paste0(script_dir, "randForest_class_wrapper.R"))
source(paste0(script_dir, "probit_flexBART_wrapper.R"))
### simulation settings ###
args = commandArgs(TRUE)
job_id = as.numeric(args[1]) + 1
source(paste0(script_dir, "settings_classification.R"))
